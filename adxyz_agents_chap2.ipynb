{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGENT #\n",
    "\n",
    "An agent, as defined in 2.1 is anything that can perceive its <b>environment</b> through sensors, and act upon that environment through actuators based on its <b>agent program</b>. This can be a dog, robot, or even you. As long as you can perceive the environment and act on it, you are an agent. This notebook will explain how to implement a simple agent, create an environment, and create a program that helps the agent act on the environment based on its percepts.\n",
    "\n",
    "Before moving on, review the </b>Agent</b> and </b>Environment</b> classes in <b>[agents.py](https://github.com/aimacode/aima-python/blob/master/agents.py)</b>.\n",
    "\n",
    "Let's begin by importing all the functions from the agents.py module and creating our first agent - a blind dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from agents import *\n",
    "\n",
    "#class BlindDog(Agent):\n",
    "#    def eat(self, thing):\n",
    "#        print(\"Dog: Ate food at {}.\".format(self.location))\n",
    "#            \n",
    "#    def drink(self, thing):\n",
    "#        print(\"Dog: Drank water at {}.\".format( self.location))\n",
    "#\n",
    "#dog = BlindDog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have just done is create a dog who can only feel what's in his location (since he's blind), and can eat or drink. Let's see if he's alive..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(dog.alive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--- \n",
    "![Cool dog](https://gifgun.files.wordpress.com/2015/07/wpid-wp-1435860392895.gif) This is our dog. How cool is he? Well, he's hungry and needs to go search for food. For him to do this, we need to give him a program. But before that, let's create a park for our dog to play in.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT #\n",
    "\n",
    "A park is an example of an environment because our dog can perceive and act upon it. The <b>Environment</b> class in agents.py is an abstract class, so we will have to create our own subclass from it before we can use it. The abstract class must contain the following methods:\n",
    "\n",
    "<li><b>percept(self, agent)</b> - returns what the agent perceives</li>\n",
    "<li><b>execute_action(self, agent, action)</b> - changes the state of the environment based on what the agent does.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class Food(Thing):\n",
    "#    pass\n",
    "\n",
    "#class Water(Thing):\n",
    "#    pass\n",
    "\n",
    "#class Park(Environment):\n",
    "#    def percept(self, agent):\n",
    "#        '''prints & return a list of things that are in our agent's location'''\n",
    "#        things = self.list_things_at(agent.location)\n",
    "#        print(things)\n",
    "#        return things\n",
    "    \n",
    "#    def execute_action(self, agent, action):\n",
    "#        '''changes the state of the environment based on what the agent does.'''\n",
    "#        if action == \"move down\":\n",
    "#            agent.movedown()\n",
    "#        elif action == \"eat\":\n",
    "#            items = self.list_things_at(agent.location, tclass=Food)\n",
    "#            if len(items) != 0:\n",
    "#                if agent.eat(items[0]): #Have the dog pick eat the first item\n",
    "#                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "#        elif action == \"drink\":\n",
    "#            items = self.list_things_at(agent.location, tclass=Water)\n",
    "#            if len(items) != 0:\n",
    "#                if agent.drink(items[0]): #Have the dog drink the first item\n",
    "#                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "                    \n",
    "#    def is_done(self):\n",
    "#        '''By default, we're done when we can't find a live agent, \n",
    "#        but to prevent killing our cute dog, we will or it with when there is no more food or water'''\n",
    "#        no_edibles = not any(isinstance(thing, Food) or isinstance(thing, Water) for thing in self.things)\n",
    "#        dead_agents = not any(agent.is_alive() for agent in self.agents)\n",
    "#        return dead_agents or no_edibles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wumpus Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from ipythonblocks import BlockGrid\n",
    "#from agents import *\n",
    "\n",
    "#color = {\"Breeze\": (225, 225, 225),\n",
    "#        \"Pit\": (0,0,0),\n",
    "#        \"Gold\": (253, 208, 23),\n",
    "#        \"Glitter\": (253, 208, 23),\n",
    "#        \"Wumpus\": (43, 27, 23),\n",
    "#        \"Stench\": (128, 128, 128),\n",
    "#        \"Explorer\": (0, 0, 255),\n",
    "#        \"Wall\": (44, 53, 57)\n",
    "#        }\n",
    "\n",
    "#def program(percepts):\n",
    "#    '''Returns an action based on it's percepts'''\n",
    "#    print(percepts)\n",
    "#    return input()\n",
    "\n",
    "#w = WumpusEnvironment(program, 7, 7)         \n",
    "#grid = BlockGrid(w.width, w.height, fill=(123, 234, 123))\n",
    "\n",
    "#def draw_grid(world):\n",
    "#    global grid\n",
    "#    grid[:] = (123, 234, 123)\n",
    "#    for x in range(0, len(world)):\n",
    "#        for y in range(0, len(world[x])):\n",
    "#            if len(world[x][y]):\n",
    "#                grid[y, x] = color[world[x][y][-1].__class__.__name__]\n",
    "\n",
    "#def step():\n",
    "#    global grid, w\n",
    "#    draw_grid(w.get_world())\n",
    "#    grid.show()\n",
    "#    w.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PROGRAM #\n",
    "Now that we have a <b>Park</b> Class, we need to implement a <b>program</b> module for our dog. A program controls how the dog acts upon it's environment. Our program will be very simple, and is shown in the table below.\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><b>Percept:</b> </td>\n",
    "        <td>Feel Food </td>\n",
    "        <td>Feel Water</td>\n",
    "        <td>Feel Nothing</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "       <td><b>Action:</b> </td>\n",
    "       <td>eat</td>\n",
    "       <td>drink</td>\n",
    "       <td>move up</td>\n",
    "   </tr>\n",
    "        \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class BlindDog(Agent):\n",
    "#    location = 1\n",
    "    \n",
    "#    def movedown(self):\n",
    "#        self.location += 1\n",
    "        \n",
    "#    def eat(self, thing):\n",
    "#        '''returns True upon success or False otherwise'''\n",
    "#        if isinstance(thing, Food):\n",
    "#            print(\"Dog: Ate food at {}.\".format(self.location))\n",
    "#            return True\n",
    "#        return False\n",
    "    \n",
    "#    def drink(self, thing):\n",
    "#        ''' returns True upon success or False otherwise'''\n",
    "#        if isinstance(thing, Water):\n",
    "#            print(\"Dog: Drank water at {}.\".format(self.location))\n",
    "#            return True\n",
    "#        return False\n",
    "        \n",
    "#def program(percepts):\n",
    "#    '''Returns an action based on it's percepts'''\n",
    "#    for p in percepts:\n",
    "#        if isinstance(p, Food):\n",
    "#            return 'eat'\n",
    "#        elif isinstance(p, Water):\n",
    "#            return 'drink'\n",
    "#    return 'move down'               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#park = Park()\n",
    "#dog = BlindDog(program)\n",
    "#dogfood = Food()\n",
    "#water = Water()\n",
    "#park.add_thing(dog, 0)\n",
    "#park.add_thing(dogfood, 5)\n",
    "#park.add_thing(water, 7)\n",
    "\n",
    "#park.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's how easy it is to implement an agent, its program, and environment. But that was a very simple case. What if our environment was 2-Dimentional instead of 1? And what if we had multiple agents?\n",
    "\n",
    "To make our Park 2D, we will need to make it a subclass of <b>XYEnvironment</b> instead of Environment. Also, let's add a person to play fetch with the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#class Park(XYEnvironment):\n",
    "#    def percept(self, agent):\n",
    "#        '''prints & return a list of things that are in our agent's location'''\n",
    "#        things = self.list_things_at(agent.location)\n",
    "#        print(things)\n",
    "#        return things\n",
    "    \n",
    "#    def execute_action(self, agent, action):\n",
    "#        '''changes the state of the environment based on what the agent does.'''\n",
    "#        if action == \"move down\":\n",
    "#            agent.movedown()\n",
    "#        elif action == \"eat\":\n",
    "#            items = self.list_things_at(agent.location, tclass=Food)\n",
    "#            if len(items) != 0:\n",
    "#                if agent.eat(items[0]): #Have the dog pick eat the first item\n",
    "#                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "#        elif action == \"drink\":\n",
    "#            items = self.list_things_at(agent.location, tclass=Water)\n",
    "#            if len(items) != 0:\n",
    "#                if agent.drink(items[0]): #Have the dog drink the first item\n",
    "#                    self.delete_thing(items[0]) #Delete it from the Park after.\n",
    "                    \n",
    "#    def is_done(self):\n",
    "#        '''By default, we're done when we can't find a live agent, \n",
    "#        but to prevent killing our cute dog, we will or it with when there is no more food or water'''\n",
    "#        no_edibles = not any(isinstance(thing, Food) or isinstance(thing, Water) for thing in self.things)\n",
    "#        dead_agents = not any(agent.is_alive() for agent in self.agents)\n",
    "#        return dead_agents or no_edibles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.7)  Write pseudocode for the goal-based and utility based agents:\n",
    "\n",
    "** Goal-based: **\n",
    "\n",
    "    currentDeltaAction=0\n",
    "    currentBestAction=[]\n",
    "    While goal==false:\n",
    "        for iAction in listOfActions:\n",
    "            if deltaValue(iAction)>currentDeltaAction:\n",
    "                currentBestAction=iAction\n",
    "        agent_action(currentBestAction):\n",
    "    \n",
    "\n",
    "** Utility-based: **\n",
    "\n",
    "    currentDeltaUtility=0\n",
    "    currentBestAction=[]\n",
    "    While true:\n",
    "        if iAction in listOfActions:\n",
    "            if deltaUtility(iAction)>currentBestUtility:\n",
    "                currentBestAction=iAction\n",
    "            \n",
    "        agent_action(currentBestAction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.8) Implement a performance-measuring environment simulator for the vacuum-cleaner world depicted in Figure 2.2 and specified on page 38.  Your implementation should be modular so that the sensors, actuators, and enviroment characteristics (size, shape, dirt placement, etc.) can be changed easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Name:  Vacuum Robot Agent\n",
    "-------------------------------\n",
    "*Performance Measure:*  +1 point for each clean square at each time step, for 1000 time steps\n",
    "\n",
    "*Environment:*  Two squares at positions (0,0) and (1,0).  The squares can either be dirty or clean.  The agent cannot go outside those two positions.\n",
    "\n",
    "*Actuators:*  The actuators for the agent consist of the ability to move between the squares and the ability to suck up dirt.\n",
    "\n",
    "*Sensors:*  The sensors allow for the agent to know current location and also whether there is dirt or not at the square the currently occupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agents import *\n",
    "\n",
    "# Define the dirt clump class\n",
    "class DirtClump(Thing):\n",
    "    pass\n",
    "\n",
    "#Define the environment class\n",
    "class adxyz_VacuumEnvironment(XYEnvironment):\n",
    "\n",
    "# Need to override the percept method \n",
    "    def percept(self, agent):\n",
    "        print ()\n",
    "        print (\"In adxyz_VacuumEnvironment - percept override:\")\n",
    "        print (\"Self = \", self)\n",
    "        print (\"Self.things = \", self.things)\n",
    "        print (\"Agent ID = \", agent)\n",
    "        print (\"Agent location = \", agent.location)\n",
    "        print (\"Agent performance = \", agent.performance)\n",
    "        \n",
    "        for iThing in self.things:\n",
    "            if iThing.location==agent.location:  #check location\n",
    "                if iThing != agent:  # Don't return agent information\n",
    "                    if (isinstance(iThing, DirtClump)):\n",
    "                        print (\"A thing which is not agent, but a dirt clump = \", iThing )\n",
    "                        print (\"Location = \", iThing.location)\n",
    "                        return agent.location, \"DirtClump\"\n",
    "                    \n",
    "        return agent.location, \"CleanSquare\"  #Default, if we don't find a dirt clump.\n",
    "                \n",
    "# Need to override the action method (and update performance measure.)\n",
    "    def execute_action(self, agent, action):\n",
    "        print ()\n",
    "        print (\"In adxyz_VacuumEnvironment - execute_action override:\")\n",
    "        print(\"self = \", self)\n",
    "        print(\"agent = \", agent)\n",
    "        print(\"current agent action = \", action)\n",
    "        print()\n",
    "        if action==\"Suck\":\n",
    "            print(\"Action-Suck\")\n",
    "            print(\"Need to remove dirt clump at correct location\")\n",
    "            deleteList = []\n",
    "            for iThing in self.things:\n",
    "                if iThing.location==agent.location:  #check location\n",
    "                    if (isinstance(iThing, DirtClump)):  # Only suck dirt\n",
    "                        print (\"A thing which is not agent, but a dirt clump = \", iThing)\n",
    "                        print (\"Location of dirt clod = \", iThing.location)\n",
    "                        self.delete_thing(iThing)\n",
    "                        break  # can only do one deletion per action.\n",
    "                                   \n",
    "        elif action==\"MoveRight\":\n",
    "            print(\"Action-MoveRight\")\n",
    "            print(\"agent direction before MoveRight = \", agent.direction)\n",
    "            print(\"agent location before MoveRight = \", agent.location)\n",
    "            agent.bump = False\n",
    "            agent.direction.direction = \"right\"\n",
    "            agent.bump = self.move_to(agent, agent.direction.move_forward(agent.location))\n",
    "            print(\"agent direction after MoveRight = \", agent.direction)\n",
    "            print(\"agent location after MoveRight = \", agent.location)\n",
    "            print()\n",
    "            \n",
    "        elif action==\"MoveLeft\":\n",
    "            print(\"Action-MoveLeft\")\n",
    "            print(\"agent direction before MoveLeft = \", agent.direction)\n",
    "            print(\"agent location before MoveLeft = \", agent.location)\n",
    "            agent.bump = False\n",
    "            agent.direction.direction = \"left\"\n",
    "            agent.bump = self.move_to(agent, agent.direction.move_forward(agent.location))\n",
    "            print(\"agent direction after MoveLeft = \", agent.direction)\n",
    "            print(\"agent location after MoveLeft = \", agent.location)\n",
    "            print()\n",
    "            \n",
    "        elif action==\"DoNothing\":\n",
    "            print(\"Action-DoNothing\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Action-Not Understood\")  #probably error.  Don't go to score section.\n",
    "            return\n",
    "                \n",
    "###\n",
    "### Count up number of clean squares (indirectly)\n",
    "### and add that to the agent peformance score\n",
    "###\n",
    "\n",
    "        print(\"Before dirt count update, agent.performance = \", agent.performance)\n",
    "        dirtCount=0\n",
    "        for iThing in self.things:\n",
    "            if isinstance(iThing, DirtClump):\n",
    "                dirtCount = dirtCount+1\n",
    "\n",
    "        cleanSquareCount = self.width*self.height-dirtCount \n",
    "        agent.performance=agent.performance + cleanSquareCount\n",
    "        print(\"After execute_action, agent.performance = \", agent.performance)\n",
    "        return    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.9) Implement a simple reflex agent for the vacuum environment in Exercise 2.8.  Run the environment with this agent for all possible initial dirt configurations and agent locations.  Record the performance score for each consideration and the overall average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# The program for the simple reflex agent is:\n",
    "# \n",
    "# Percept:         Action:\n",
    "# --------         -------\n",
    "# [(0,0),Clean] -> Right\n",
    "# [(0,0),Dirty] -> Suck\n",
    "# [(1,0),Clean] -> Left\n",
    "# [(1,0),Dirty] -> Suck\n",
    "#\n",
    "\n",
    "def adxyz_SimpleReflexVacuum(percept):\n",
    "     \n",
    "    if percept[0] == (0,0) and percept[1]==\"DirtClump\":\n",
    "        return \"Suck\"\n",
    "    elif percept[0] == (1,0) and percept[1]==\"DirtClump\":\n",
    "        return \"Suck\"\n",
    "    elif percept[0] == (0,0) and percept[1]==\"CleanSquare\":\n",
    "        return \"MoveRight\"\n",
    "    elif percept[0] == (1,0) and percept[1]==\"CleanSquare\":\n",
    "        return \"MoveLeft\"\n",
    "    else:\n",
    "        return \"DoNothing\" # Not sure how you would get here, but DoNothing to be safe.\n",
    "\n",
    "# Instantiate a simple reflex vacuum agent\n",
    "class adxyz_SimpleReflexVacuumAgent(Agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initDirt =  [[]]\n",
      "initAgent =  [(0, 0), (1, 0)]\n"
     ]
    }
   ],
   "source": [
    "# Define the initial dirt configurations\n",
    "initDirt=[]\n",
    "initDirt.append([])             # neither location dirty - format(X,Y)-locations:A=(0,0), B=(1,0)\n",
    "###initDirt.append([(0,0)])        # square A dirty, square B clean\n",
    "##initDirt.append([(1,0)])        # square A clean, square B dirty\n",
    "###initDirt.append([(0,0),(1,0)])  # square A dirty, square B dirty\n",
    "\n",
    "print(\"initDirt = \", initDirt)\n",
    "\n",
    "#\n",
    "# Create agent placements\n",
    "#\n",
    "initAgent=[]\n",
    "initAgent.append((0,0))\n",
    "initAgent.append((1,0))\n",
    "print(\"initAgent = \", initAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: iSimAgentPlacement =  0\n",
      "Simulation: iSimDirtPlacement =  0\n",
      "\n",
      "Environment:\n",
      "<adxyz_SimpleReflexVacuumAgent> (0, 0)\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 0\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (0, 0)\n",
      "Agent performance =  0\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveRight\n",
      "\n",
      "Action-MoveRight\n",
      "agent direction before MoveRight =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location before MoveRight =  (0, 0)\n",
      "agent direction after MoveRight =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location after MoveRight =  (1, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  0\n",
      "After execute_action, agent.performance =  2\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 1\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (1, 0)\n",
      "Agent performance =  2\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveLeft\n",
      "\n",
      "Action-MoveLeft\n",
      "agent direction before MoveLeft =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location before MoveLeft =  (1, 0)\n",
      "agent direction after MoveLeft =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location after MoveLeft =  (0, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  2\n",
      "After execute_action, agent.performance =  4\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 2\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (0, 0)\n",
      "Agent performance =  4\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveRight\n",
      "\n",
      "Action-MoveRight\n",
      "agent direction before MoveRight =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location before MoveRight =  (0, 0)\n",
      "agent direction after MoveRight =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location after MoveRight =  (1, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  4\n",
      "After execute_action, agent.performance =  6\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 3\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (1, 0)\n",
      "Agent performance =  6\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveLeft\n",
      "\n",
      "Action-MoveLeft\n",
      "agent direction before MoveLeft =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location before MoveLeft =  (1, 0)\n",
      "agent direction after MoveLeft =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location after MoveLeft =  (0, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  6\n",
      "After execute_action, agent.performance =  8\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 4\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (0, 0)\n",
      "Agent performance =  8\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b0b8>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveRight\n",
      "\n",
      "Action-MoveRight\n",
      "agent direction before MoveRight =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location before MoveRight =  (0, 0)\n",
      "agent direction after MoveRight =  <agents.Direction object at 0x10552b0f0>\n",
      "agent location after MoveRight =  (1, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  8\n",
      "After execute_action, agent.performance =  10\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<====>\n",
      "<====>\n",
      "Final performance measure for Agent =  10\n",
      "======\n",
      "======\n",
      "\n",
      "Simulation: iSimAgentPlacement =  1\n",
      "Simulation: iSimDirtPlacement =  0\n",
      "\n",
      "Environment:\n",
      "<adxyz_SimpleReflexVacuumAgent> (1, 0)\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 0\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (1, 0)\n",
      "Agent performance =  0\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveLeft\n",
      "\n",
      "Action-MoveLeft\n",
      "agent direction before MoveLeft =  <agents.Direction object at 0x10552b978>\n",
      "agent location before MoveLeft =  (1, 0)\n",
      "agent direction after MoveLeft =  <agents.Direction object at 0x10552b978>\n",
      "agent location after MoveLeft =  (0, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  0\n",
      "After execute_action, agent.performance =  2\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 1\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (0, 0)\n",
      "Agent performance =  2\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveRight\n",
      "\n",
      "Action-MoveRight\n",
      "agent direction before MoveRight =  <agents.Direction object at 0x10552b978>\n",
      "agent location before MoveRight =  (0, 0)\n",
      "agent direction after MoveRight =  <agents.Direction object at 0x10552b978>\n",
      "agent location after MoveRight =  (1, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  2\n",
      "After execute_action, agent.performance =  4\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 2\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (1, 0)\n",
      "Agent performance =  4\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveLeft\n",
      "\n",
      "Action-MoveLeft\n",
      "agent direction before MoveLeft =  <agents.Direction object at 0x10552b978>\n",
      "agent location before MoveLeft =  (1, 0)\n",
      "agent direction after MoveLeft =  <agents.Direction object at 0x10552b978>\n",
      "agent location after MoveLeft =  (0, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  4\n",
      "After execute_action, agent.performance =  6\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 3\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (0, 0)\n",
      "Agent performance =  6\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveRight\n",
      "\n",
      "Action-MoveRight\n",
      "agent direction before MoveRight =  <agents.Direction object at 0x10552b978>\n",
      "agent location before MoveRight =  (0, 0)\n",
      "agent direction after MoveRight =  <agents.Direction object at 0x10552b978>\n",
      "agent location after MoveRight =  (1, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  6\n",
      "After execute_action, agent.performance =  8\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<-START->\n",
      "Simulation: step = 4\n",
      "\n",
      "In adxyz_VacuumEnvironment - percept override:\n",
      "Self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "Self.things =  [<adxyz_SimpleReflexVacuumAgent>]\n",
      "Agent ID =  <adxyz_SimpleReflexVacuumAgent>\n",
      "Agent location =  (1, 0)\n",
      "Agent performance =  8\n",
      "\n",
      "In adxyz_VacuumEnvironment - execute_action override:\n",
      "self =  <__main__.adxyz_VacuumEnvironment object at 0x10552b940>\n",
      "agent =  <adxyz_SimpleReflexVacuumAgent>\n",
      "current agent action =  MoveLeft\n",
      "\n",
      "Action-MoveLeft\n",
      "agent direction before MoveLeft =  <agents.Direction object at 0x10552b978>\n",
      "agent location before MoveLeft =  (1, 0)\n",
      "agent direction after MoveLeft =  <agents.Direction object at 0x10552b978>\n",
      "agent location after MoveLeft =  (0, 0)\n",
      "\n",
      "Before dirt count update, agent.performance =  8\n",
      "After execute_action, agent.performance =  10\n",
      "---END---\n",
      "---------\n",
      "\n",
      "\n",
      "<====>\n",
      "<====>\n",
      "Final performance measure for Agent =  10\n",
      "======\n",
      "======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a loop over environments to run simulation\n",
    "\n",
    "# Loop over agent placements\n",
    "for iSimAgentPlacement in range(len(initAgent)):\n",
    "###for iSimAgentPlacement in range(1):\n",
    "    print(\"Simulation: iSimAgentPlacement = \", iSimAgentPlacement)\n",
    "\n",
    "# Loop over dirt placements\n",
    "    for iSimDirtPlacement in range(len(initDirt)):\n",
    "        print (\"Simulation: iSimDirtPlacement = \" , iSimDirtPlacement)\n",
    "        myVacEnv = adxyz_VacuumEnvironment() #Create a new environment for each dirt/agent setup\n",
    "        myVacEnv.width = 2\n",
    "        myVacEnv.height = 1\n",
    "\n",
    "        for iPlace in range(len(initDirt[iSimDirtPlacement])):\n",
    "            print (\"Simulation: iPlace = \" , iPlace)\n",
    "            myVacEnv.add_thing(DirtClump(),location=initDirt[iSimDirtPlacement][iPlace])\n",
    "            \n",
    "#\n",
    "# Now setup the agent.\n",
    "#\n",
    "        myAgent=adxyz_SimpleReflexVacuumAgent()\n",
    "        myAgent.program=adxyz_SimpleReflexVacuum  #Place the agent program here\n",
    "        myAgent.performance=0\n",
    "\n",
    "# Instantiate a direction object for 2D generality\n",
    "        myAgent.direction = Direction(\"up\")  # need to leverage heading mechanism\n",
    "        \n",
    "# Add agent to environment\n",
    "        myVacEnv.add_thing(myAgent,location=initAgent[iSimAgentPlacement])\n",
    "        print()\n",
    "        print(\"Environment:\")\n",
    "        for iThings in myVacEnv.things:\n",
    "            print(iThings, iThings.location)\n",
    "        print()\n",
    "        \n",
    "#\n",
    "# Now step the environment clock\n",
    "#\n",
    "        numSteps = 5\n",
    "        for iStep in range(numSteps):\n",
    "            print()\n",
    "            print(\"<-START->\")\n",
    "            print(\"Simulation: step =\", iStep)\n",
    "            myVacEnv.step()\n",
    "            print(\"---END---\")\n",
    "            print(\"---------\")\n",
    "            print()\n",
    "            \n",
    "        print()    \n",
    "        print(\"<====>\")\n",
    "        print(\"<====>\")\n",
    "        #need to keep running tally of initial configuration and final performance\n",
    "        print(\"Final performance measure for Agent = \", myAgent.performance)\n",
    "        print(\"======\")\n",
    "        print(\"======\")\n",
    "        print()\n",
    "#\n",
    "# End of script\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo:\n",
    "- Clean up comments/prints (mostly done)\n",
    "- Make processing more generalized\n",
    "-- Introduce multiple dirt clods.\n",
    "-- Introduce multiple agents.\n",
    "- Move data to cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.10) Consider the modified version of the performance metric where the agent is penalized on point for each movement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Can a simple reflex agent be perfectly rational for this environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, there are 8 cases to consider (8 states of the environment):  4 initial dirt configurations and 2 initial agent configurations.\n",
    "\n",
    "Case 1a) Clean A, Clean B, agent in square A: The maximum performance score would be 2 points awarded at each step, because there are two clean squares.  If we were to design a reflex agent, we could use the following program:  [(clean, squareA)-->DoNothing]\n",
    "Case 1b) Clean A, Clean B, agent in square B: The maximum performance score would be 2 points awarded at each step, because there are two clean squares.  If we were to design a reflex agent, we could use the following program: [(clean, SquareB)-->DoNothing]\n",
    "\n",
    "Case 2a) Dirt A, Clean B, agent in square A:  The maximum performance score would be 2 points, once the dirt is removed from square A.  The agent program that could accomplish this is:  [(dirt, squareA)-->suck], [(clean, squareA)-->DoNothing]\n",
    "Case 2b) Dirt A, Clean B, agent in square B:  The maximum performance score would be 1-1 (1 point for clean B, -1 for move to A), then 2 points for each step after that.  The agent program that could accomplish this is: [(clean, squareB)-->MoveLeft], [(dirt, squareA)-->suck], [(clean, squareA)-->DoNothing].  However, this is in conflict with the optimum program for Case 1b.\n",
    "\n",
    "Case 3a) Clean A, Dirt B, agent in squareA:  The maximum peformance score would be 1(for clean initial square) -1 (for move to B) = 0 points for step 1.  2 points each step from then on.  The agent program that could accomplish this would be:  [(clean, squareA)-->MoveRight], [(Dirt, SquareB)-->Suck], [(clean,SquareB)-->doNothing].  However, we can see from this situation that our program for 3a is in conflict with the program for 1a.\n",
    "Case 3b) Clean A, Dirt B, agent in squareB:  The maximum performance score for this would be 2 per time step:  The following agent program could accomplish this [(Dirt, SquareB)-->Suck][(clean,SquareB)-->doNothing.\n",
    "\n",
    "Case 4a) Dirt A, Dirt B, Agent in Square A:  The maximum possible performance points would be 1 for first step, 1-1 for second step, 2 points from that step onwards.  An agent function that could accomplish this is:  [(dirt,squareA)-->suck], [(clean,squareA)-->moveRight], [(dirt,SquareB)-->suck], [(clean,SquareB)-->doNothing].  However, this includes an instruction which is in conflict with the optimum program in case 1a.\n",
    "\n",
    "Case 4b) Dirt A, Dirt B, Agent in Square B:  The maximum possible performance points would be 1 for the first step, 1-1 for the second step, and 2 points from the step onwards.  An agent function that could accomplish this is:  [(dirt, squareB)-->suck], [(clean,squareB)-->moveLeft], [(dirt,squareA)-->suck], [(clean, squareA)-->doNothing].  This has instruction which are in conflict with case 1b.  \n",
    "\n",
    "Because we have conflicting instructions in order to achieve optimum performance results, we would have to choose one or the other, which would lead to a suboptimal result in at least one case.  Thus a perfectly rational agent cannot be designed.  By perfectly rational, I mean one that is optimum in every case, since we must assume all cases are possible to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What about a reflex agent with state?  Design such an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# The program for the simple reflex agent with state is:\n",
    "# \n",
    "# Percept:         Action:\n",
    "# --------         -------\n",
    "# [(0,0),Clean] -> Right\n",
    "# [(0,0),Dirty] -> Suck\n",
    "# [(1,0),Clean] -> Left\n",
    "# [(1,0),Dirty] -> Suck\n",
    "#\n",
    "\n",
    "def adxyz_SimpleReflexStateVacuum(percept):\n",
    "     \n",
    "    if percept[0] == (0,0) and percept[1]==\"DirtClump\":\n",
    "        return \"Suck\"\n",
    "    elif percept[0] == (1,0) and percept[1]==\"DirtClump\":\n",
    "        return \"Suck\"\n",
    "    elif percept[0] == (0,0) and percept[1]==\"CleanSquare\":\n",
    "        return \"MoveRight\"\n",
    "    elif percept[0] == (1,0) and percept[1]==\"CleanSquare\":\n",
    "        return \"MoveLeft\"\n",
    "    else:\n",
    "        return \"DoNothing\" # Not sure how you would get here, but DoNothing to be safe.\n",
    "\n",
    "# Instantiate a simple reflex vacuum agent\n",
    "class adxyz_SimpleReflexStateVacuumAgent(Agent):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking to design agents that can solve goal seeking problems.\n",
    "Step 1:  Define the goal, which is a state of the environment.  For example, the desired goal might be \"Car in Bucharest\" or \"Robot in square (10,10) with all squares clean\"  \n",
    "Step 2:  Define the problem.  \n",
    "- Define the states of the environment (atomic)\n",
    "- Define the initial state\n",
    "- Define legal actions\n",
    "- Define transitions\n",
    "- Define goal test\n",
    "- Define path/step costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graph-search:  A key algorithm for expanding the search space, that avoids redundent paths.  The search methods in this chapter are based on graph-search algorithm.\n",
    "Each step of the algorithm does this:\n",
    "Unexplored state -> frontier states -> explored states.\n",
    "A state can only be in one of the three above categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infrastructure for search algorithms:\n",
    "Graphs - nodes that include references to \n",
    "parent nodes\n",
    "state descriptions\n",
    "action that got from parent to child node\n",
    "path cost (from initial state).\n",
    "\n",
    "Types of cost:\n",
    "Search cost (time to determine solution)\n",
    "Path cost (cost of actual solution - for example distance on a roadmap)\n",
    "Total cost:  Sum of search + path cost (with appropriate scaling to put them in common units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Search Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm evaluation criteria:\n",
    "- Completeness (Does the algorithm find a solution - or all solutions)\n",
    "- Optimality (Does the algorithm find the best solution)\n",
    "- Time complexity (how long does the algorithm take to find solution)\n",
    "- Space complexity (how much memory is used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uninformed search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes all search algorithms that have no idea whether one choice is \"more promising\" than another non-goal state.  These algorithms generate non-goal states and test for goal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Breadth-first search:  Each node is expanded into the successor nodes one level at a time.  Uses a FIFO queue for the frontier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code for BFS search:\n",
    "\n",
    "    unexploredNodes = {AllStates}-initialState\n",
    "    exploredNodes = {}\n",
    "    frontierNodes = initialState\n",
    "    \n",
    "        currentNode = frontierNodes.pop\n",
    "        if currentNode == goalState:\n",
    "            break\n",
    "        else:\n",
    "            exploredNodes.push(currentNode)\n",
    "            for childNode in currentNode.children:\n",
    "                frontierNodes.push(childNode)\n",
    "                unexploredNodes.delete(childNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1 ID =  <__main__.searchNode object at 0x105555898>\n",
      "node2 ID =  <__main__.searchNode object at 0x105555908>\n",
      "node3 ID =  <__main__.searchNode object at 0x105555940>\n",
      "\n",
      "node1 children =  [<__main__.searchNode object at 0x105555908>, <__main__.searchNode object at 0x105555940>]\n",
      "node2 parent =  <__main__.searchNode object at 0x105555898>\n",
      "node3 parent =  <__main__.searchNode object at 0x105555898>\n",
      "tmpQueue =  <__main__.searchNode object at 0x105555898>\n",
      "tmpQueue =  <__main__.searchNode object at 0x105555908>\n",
      "tmpQueue =  <__main__.searchNode object at 0x105555940>\n"
     ]
    }
   ],
   "source": [
    "# define a search node class\n",
    "class searchNode():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parent=None\n",
    "        self.child=[]\n",
    "        self.pathCost=0\n",
    "        self.state=None\n",
    "    \n",
    "    def setParent(self, parentNode):\n",
    "        self.parent=parentNode\n",
    "        return\n",
    "\n",
    "    def setState(self, state):\n",
    "        self.state=state\n",
    "        return\n",
    "    \n",
    "    def setChild(self, childNode):\n",
    "        self.child.append(childNode)\n",
    "        childNode.setParent(self)\n",
    "        return\n",
    "    \n",
    "    def setPathCost(self, stepCost):\n",
    "        self.pathCost=self.pathCost+stepCost\n",
    "        return\n",
    "    \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "\n",
    "    def getState(self):\n",
    "        return self.state\n",
    "    \n",
    "    def getChild(self):\n",
    "        return self.child\n",
    "    \n",
    "    def getPathCost(self):\n",
    "        return self.pathCost\n",
    "    \n",
    "#\n",
    "# Instantiate a few nodes\n",
    "#\n",
    "\n",
    "node1 = searchNode()\n",
    "node2 = searchNode()\n",
    "node3 = searchNode()\n",
    "\n",
    "node1.setChild(node2)\n",
    "node1.setChild(node3)\n",
    "\n",
    "#\n",
    "# Check out hierarchy so far\n",
    "#\n",
    "print(\"node1 ID = \", node1)\n",
    "print(\"node2 ID = \", node2)\n",
    "print(\"node3 ID = \", node3)\n",
    "print()\n",
    "\n",
    "print(\"node1 children = \", node1.getChild())\n",
    "print(\"node2 parent = \", node2.getParent())\n",
    "print(\"node3 parent = \", node3.getParent())\n",
    "\n",
    "#\n",
    "# Instantiate a queue\n",
    "#\n",
    "\n",
    "import queue\n",
    "\n",
    "frontierQueue = queue.Queue()\n",
    "\n",
    "frontierQueue.put(node1)\n",
    "frontierQueue.put(node2)\n",
    "frontierQueue.put(node3)\n",
    "\n",
    "print(\"Exploring frontier queue:\")\n",
    "while not frontierQueue.empty():\n",
    "    tmp = frontierQueue.get()\n",
    "    print(\"tmpQueue = \", tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informed search (heuristics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
